{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b151c8f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github.com/mazzulab/ssm/blob/master/notebooks/dyn%20sys%20tutorial/Input-driven%20linear%20model%20(LM-HMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installing repo and dependencies if using colab (restart Colab Runtime after installing). Skip to imports if running locally.\n",
    "\n",
    "!pip install -U matplotlib &> /dev/null\n",
    "!git clone https://github.com/mazzulab/ssm &> /dev/null\n",
    "!pip install git+https://github.com/mazzulab/ssm.git &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063ceac",
   "metadata": {},
   "source": [
    "1. Load mystery 2d dataset of $(R_i,F_i)$ pairs, for $i=1,\\ldots,N$.\n",
    "\n",
    "2. Fit a Linear Model (LM)\n",
    "\n",
    "3. Fit mixture of K Linear Models (MLM)\n",
    "- Find the best K for MLM (model selection)\n",
    "- Cross-validation\n",
    "- BIC, AIC criteria\n",
    "\n",
    "5. Is there a long timescale in the dataset?\n",
    "\n",
    "6. Hidden Markov model with Linear Model emissions (LM-HMM)\n",
    "- Generative mode: instantiate model with K=3 states, set model parameters, sample data from synthetic model\n",
    "- Parameter recovery\n",
    "- Model selection: find the best K that fits ground truth (K=3) \n",
    "\n",
    "4. Fit LM-HMM to mystery data\n",
    "- Model selection: Find best K that fits mystery data\n",
    "- Fit model to data\n",
    "- compare LM, MLM, LM-HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0368083",
   "metadata": {},
   "source": [
    "# Generate mystery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d501587f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpr\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mssm\u001b[39;00m \u001b[38;5;66;03m# note this should be the forked ssm repo above\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "np.random.seed(2)\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import ssm # note this should be the forked ssm repo above\n",
    "from ssm.util import find_permutation\n",
    "import ssm.utils_tutorial as utils_tutorial\n",
    "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "color_names = [\"windows blue\",\n",
    "               \"red\",\n",
    "               \"amber\",\n",
    "               \"faded green\",\n",
    "               \"dusty purple\",\n",
    "               \"orange\",\n",
    "               \"clay\",\n",
    "               \"pink\",\n",
    "               \"greyish\",\n",
    "               \"mint\",\n",
    "               \"cyan\",\n",
    "               \"steel blue\",\n",
    "               \"forest green\",\n",
    "               \"pastel purple\",\n",
    "               \"salmon\",\n",
    "               \"dark brown\"]\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "cmap = gradient_cmap(colors)\n",
    "from ssm import utilplot\n",
    "\n",
    "# Speficy whether or not to save figures\n",
    "save_figures = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "obs, inpt, mystery_model,true_labels = utils_tutorial.mysteryData()\n",
    "data={'inpt':inpt, 'obs':obs}\n",
    "\n",
    "# Save the dictionary\n",
    "# with open('mysteryData.pkl', 'wb') as f: pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4bdd7",
   "metadata": {},
   "source": [
    "## Fit a Linear Model (LM)\n",
    "\n",
    "- estimate hold-out R2 (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# with open('mysteryData.pkl', 'rb') as file:\n",
    "#     new_data = pickle.load(file)\n",
    "\n",
    "# Unpacking the new data\n",
    "inpt_new = data['inpt']\n",
    "obs_new = data['obs']\n",
    "\n",
    "# Fit a simple linear regression model\n",
    "X_new = inpt_new\n",
    "y_new = obs_new\n",
    "model_new = LinearRegression()\n",
    "\n",
    "model_tot = LinearRegression()\n",
    "model_tot.fit(X_new, y_new)\n",
    "y_pred=model_tot.predict(X_new)\n",
    "r2_new=model_tot.score(y_pred, y_new)\n",
    "\n",
    "# Scatter plot of the data and the linear regression model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_new, y_new, label='Data points')\n",
    "plt.plot(X_new, model_tot.predict(X_new), color='red', label=f'LM r2={r2_new:.3f}')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Observation')\n",
    "plt.title('Scatter plot with Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a349c",
   "metadata": {},
   "source": [
    "## Fit a mixture of linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b35e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "n_components=2\n",
    "# Load the provided data\n",
    "# file_path_new = 'mysteryData.pkl'\n",
    "# with open(file_path_new, 'rb') as file:\n",
    "#     new_data = pickle.load(file)\n",
    "\n",
    "# Unpack the new data\n",
    "inpt_new = np.ravel(data['inpt'])\n",
    "obs_new = np.ravel(data['obs'])\n",
    "\n",
    "# Reshape the data\n",
    "X = inpt_new.reshape(-1, 1)\n",
    "y = obs_new\n",
    "\n",
    "# Fit the model to the generated data\n",
    "model = utils_tutorial.MixtureOfLinearRegressions(n_components=n_components)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X,y)\n",
    "r2_mlm = r2_score(y, predictions)\n",
    "\n",
    "# Predict labels for each data point\n",
    "labels_em = model.predict_labels(X,y)\n",
    "\n",
    "# Scatter plot of the data with EM components and fitted lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(n_components):\n",
    "    plt.scatter(X[labels_em==i], y[labels_em==i], color=colors[i],alpha=0.3, label='Data points')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Observation')\n",
    "plt.title('Scatter plot with Mixture of Linear Regressions using EM')\n",
    "\n",
    "# Overlay the fitted lines\n",
    "for i, lin_model in enumerate(model.em_algorithm.models_):\n",
    "    plt.plot(X, lin_model.predict(X), color=colors[i], label=f'LM {i+1}')\n",
    "plt.title(f'r2={r2_mlm:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f4483",
   "metadata": {},
   "source": [
    "## Find best number of hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def cross_validate_model(n_components, X, y, kf):\n",
    "    r2_scores_em = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model_cv = utils_tutorial.MixtureOfLinearRegressions(n_components=n_components)\n",
    "        model_cv.fit(X_train, y_train)\n",
    "        predictions_test = model_cv.predict(X_test,y_test)\n",
    "\n",
    "        r2_score_fold = r2_score(y_test, predictions_test)\n",
    "        r2_scores_em.append(r2_score_fold)\n",
    "    \n",
    "    return np.mean(r2_scores_em)\n",
    "\n",
    "# Perform cross-validated model selection to find the best number of mixture components\n",
    "component_range = range(1, 5)\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "mean_cv_r2_scores = Parallel(n_jobs=100)(delayed(cross_validate_model)(n_components, X, y, kf) for n_components in component_range)\n",
    "\n",
    "# Plot the average hold out R2 as a function of the mixture components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(component_range, mean_cv_r2_scores, marker='o')\n",
    "plt.xlabel('Number of Mixture Components')\n",
    "plt.ylabel('Average Hold Out R2')\n",
    "plt.title('Average Hold Out R2 vs. Number of Mixture Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "mean_cv_r2_scores\n",
    "\n",
    "best_k=utils_tutorial.find_elbow_point(mean_cv_r2_scores)\n",
    "print(f'The best number of mixture components is {best_k}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64894a07",
   "metadata": {},
   "source": [
    "## Plot time series of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the generated data\n",
    "n_components=best_k\n",
    "model = utils_tutorial.MixtureOfLinearRegressions(n_components=n_components)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict responsibilities and labels\n",
    "responsibilities = model.em_algorithm.responsibilities_\n",
    "posterior_probs=responsibilities\n",
    "predicted_labels = model.predict_labels(X,y)\n",
    "xlim=400\n",
    "# Plot the time series of the data (obs and inputs)\n",
    "plt.figure(figsize=(14, 8))\n",
    "n_samples = len(y)\n",
    "# Plot observations and inputs\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(n_samples), y, label='Observations', color='black')\n",
    "plt.plot(range(n_samples), X.flatten(), label='Inputs', color='gray')\n",
    "\n",
    "for i in range(n_components):\n",
    "    plt.fill_between(range(n_samples), y, where=(posterior_probs[:, i] > 0.5), color=colors[i], alpha=0.3, label=f'Mixture Component {i+1}')\n",
    "\n",
    "plt.xlabel('Time (Sample Number)')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Time Series of Observations and Inputs with posterior_probs')\n",
    "plt.xlim(0,xlim)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot posterior_probs\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(n_components):\n",
    "    plt.plot(range(n_samples), posterior_probs[:, i], label=f'posterior_probs {i+1}', color=colors[i])\n",
    "\n",
    "plt.xlabel('Time (Sample Number)')\n",
    "plt.ylabel('Responsibility')\n",
    "plt.title('posterior_probs of Each Mixture Component Over Time')\n",
    "plt.xlim(0,xlim)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the predicted labels\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "indplot = np.arange(n_samples)\n",
    "for i in range(n_components):\n",
    "    mask = (predicted_labels == i)\n",
    "    plt.scatter(indplot[mask], y[mask], color=colors[i], label=f'Label {i}')\n",
    "plt.xlabel('Time (Sample Number)')\n",
    "plt.ylabel('Observations')\n",
    "plt.title('Predicted Labels for Each Sample')\n",
    "plt.xlim(0,xlim)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "for i in range(posterior_probs.shape[1]):\n",
    "    plt.subplot(1,posterior_probs.shape[1],  i+1)\n",
    "    plt.hist(posterior_probs[:, i], color=colors[i], label=f'p={i+1}')\n",
    "    plt.legend()\n",
    "    plt.title(f'posterior_probs {i+1}')\n",
    "    plt.xlabel('posterior_probs'); plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6b6af",
   "metadata": {},
   "source": [
    "## Plot autocorrelation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a40845",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 6))\n",
    "xlim=300\n",
    "timescale = []\n",
    "for i in range(n_components):\n",
    "          y_acf=utils_tutorial.autocovariance(responsibilities[:,i])\n",
    "          y_acf=y_acf[1:]/np.max(y_acf[1:])\n",
    "          timescale.append( int(utils_tutorial.compute_hwhm(y_acf)))\n",
    "          plt.plot(y_acf, label=f\"Timescale: {timescale[i]}\")\n",
    "          plt.axhline(y_acf[timescale[i]], xmin=0, xmax=(timescale[i])/xlim, color='red', linestyle='--')\n",
    "y_acf=utils_tutorial.autocovariance(y)\n",
    "y_acf=y_acf[1:]/np.max(y_acf[1:])\n",
    "timescale.append( int(utils_tutorial.compute_hwhm(y_acf)))\n",
    "plt.plot(y_acf, label=f\"Timescale: {timescale[i+1]}\")\n",
    "plt.axhline(y_acf[timescale[i+1]], xmin=0, xmax=(timescale[i+1])/xlim, color='red', linestyle='--')\n",
    "plt.axhline(0, color='black')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.title(\"Autocorrelation of Time Series with Timescales\")\n",
    "plt.xlim([0, xlim])\n",
    "plt.show()\n",
    "# plt.savefig('AutocorrTimescales.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb5499",
   "metadata": {},
   "source": [
    "# Hidden Markov model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab898f66-00bf-47f7-80f3-d3c7f5cb5731",
   "metadata": {},
   "source": [
    "**LM-HMM with input-driven Gaussian emissions** \n",
    "\n",
    "*This notebook demonstrates the model published in [(Cazettes et al., Nat. Neuro. 2023)](https://www.nature.com/articles/s41593-023-01305-8) and was prepared by Luca Mazzucato (University of Oregon). It is part of this [forked ssm repo](https://github.com/mazzulab/ssm/blob/master/notebooks/2c%20Input-driven%20linear%20model%20(LM-HMM).ipynb). Part of the code is adapted from [`2b Input Driven Observations (GLM-HMM)`](https://github.com/mazzulab/ssm/blob/master/notebooks/2b%20Input%20Driven%20Observations%20(GLM-HMM).ipynb), by [(Ashwood et al., Nat. Neuro. 2022)](https://www.nature.com/articles/s41593-021-01007-z).  Please ask questions at lmazzuca at uoregon dot edu.*\n",
    "\n",
    "In order to run this notebook, please install the conda environment [`ssm_conda_mazzu`](https://github.com/mazzulab/AppliedMathNotebooks/blob/main/setup_conda_env_ssm_conda.sh).\n",
    "\n",
    "Let us start from a simple linear model (LM) where some dependent variables $y_i$ (the observations, $i=1,\\ldots,D=$`obs_dim`) are generated from some dependent variables $x_m$ (the inputs, $m=1,\\ldots,M=$`input_dim`) based on a linear relationship (corresponding to a gaussian multivariante regression) \n",
    "$$\n",
    "y_i=\\sum_{m=1}^M W^{(k)}_{im}x_m+\\mu^{(k)}_i+\\epsilon_i^{(k)}\n",
    "$$\n",
    "with regression coefficients $(W^{(k)}_{im},\\mu^{(k)}_i)$ and gaussian noise $\\epsilon_i\\sim{\\it N}(0,\\Sigma^{(k)}_{ij})$, with noise covariance $\\Sigma_{ij}$. In the case $k=1$, we have only one set of regression coefficients and $(W^{(k)},\\mu^{(k)})$ and this corresponds to the standard linear regression problem.\n",
    "\n",
    "Now let us assume that the dependence of $y$ on $x$ varies with time and takes values among a discrete set of $k=1,\\ldots,K=$=`num_states` different gaussian LMs, which evolve according to an underlying Markov chain. Each LM consists of the parameter set $(W^{(k)}_{im},\\mu^{(k)}_i,\\Sigma^{(k)}_{ij})$, and the switching dynamics between the different LMs are governed by the underlying transition probability matrix $A_{kl}$. This is an LM-HMM with multivariate Gaussian emissions. The total set of parameters includes also the initial probabilities for each state $\\pi^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85948351-ce15-4e10-866b-beb211b544bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "npr.seed(3)\n",
    "\n",
    "import ssm # note this should be the forked ssm repo above\n",
    "from ssm.util import find_permutation\n",
    "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "color_names = [\n",
    "    \"windows blue\",\n",
    "    \"red\",\n",
    "    \"amber\",\n",
    "    \"faded green\",\n",
    "    \"dusty purple\",\n",
    "    \"orange\"\n",
    "    ]\n",
    "\n",
    "colors = sns.xkcd_palette(color_names)\n",
    "cmap = gradient_cmap(colors)\n",
    "\n",
    "from ssm import utilplot\n",
    "\n",
    "# Speficy whether or not to save figures\n",
    "save_figures = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56e4c32e",
   "metadata": {},
   "source": [
    "## 1. Generate LM-HMM observations ##\n",
    "\n",
    "### 1.a Specify parameters of the ground truth LM-HMM\n",
    "\n",
    "Here we generate observations from an LM-HMM `true_hmm` with `input_dim` dimensional inputs, `obs_dim` dimensional observations, and `num_states` hidden states. We choose specific ground truth values of the model parameters $(W^{(k)}_{im},\\mu^{(k)}_i,\\Sigma^{(k)}_{ij})$ which can be set to the variables `true_hmm.observations.Wks`, `true_hmm.observations.mus`, and `true_hmm.observations.Sigmas`, respectively. We choose the values of the transition probabilities $A_{kl}$ between hidden states as well by setting the log of the transition probability matrix `true_hmm.transitions.log_Ps`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6407f-a74e-4d67-bba9-9223756bfcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters of the HMM\n",
    "time_bins = 500   # number of time bins\n",
    "num_states = 3    # number of discrete states\n",
    "obs_dim = 1       # dimensionality of observation\n",
    "input_dim=1     # input dimension\n",
    "\n",
    "# Create an HMM\n",
    "true_hmm = ssm.HMM(num_states, obs_dim, M=input_dim,observations=\"input_driven_obs_gaussian\", transitions=\"standard\")\n",
    "# set weights Wk, biases mus, and noise covariance sigmas by hand\n",
    "\n",
    "np.random.seed(0)\n",
    "gen_weights=np.random.randn(num_states,obs_dim,input_dim)\n",
    "\n",
    "mus=np.random.randn(num_states, obs_dim)\n",
    "\n",
    "stdnoise=1\n",
    "sigma=stdnoise**2*np.eye(obs_dim) # diagonal noise correlations with variance stdnoise**2\n",
    "sigmas = np.dstack([sigma]*num_states).transpose((2,0,1))\n",
    "\n",
    "true_hmm.observations.mus = mus\n",
    "true_hmm.observations.Sigmas = sigmas\n",
    "true_hmm.observations.Wks =  2*gen_weights\n",
    "\n",
    "# set transition probabilities as well\n",
    "trans_eps=0.05 # off diag transition prob to another state\n",
    "trans0=trans_eps*np.ones((num_states,num_states))\n",
    "for i in range(num_states):\n",
    "    trans0[i,i]=1-(trans_eps*(num_states-1))   \n",
    "true_hmm.transitions.log_Ps=np.log(trans0)\n",
    "print('trans\\n',trans0)\n",
    "print('mus\\n',mus)\n",
    "print('Wks\\n',gen_weights)\n",
    "print('Sigmas\\n',sigmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generative parameters:\n",
    "mus=true_hmm.observations.mus\n",
    "gen_weights=true_hmm.observations.Wks\n",
    "\n",
    "utilplot.plot_weights(gen_weights,mus)\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_params.pdf\")\n",
    "    \n",
    "fig = plt.figure(figsize=(5, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "utilplot.plot_trans_matrix(trans0)\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_trans.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dba61e9",
   "metadata": {},
   "source": [
    "### 1.b Sample data from the LM-HMM\n",
    "\n",
    "Sample data from the LM-HMM. We create arrays of dimension (`time_bins`,`input_dim`) representing the dependent variables $x_i$, or inputs to the LM-HMM. We will generate a linearly varying input in all dimensions with slope 0.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6bdb6-2f44-4af2-b499-155dca85121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an exogenous input dim T x M\n",
    "inpt = np.random.rand(time_bins) # generate random inputs from uniform distribution\n",
    "# inpt = 0.1*(np.arange(time_bins)+1) # generate linearly increasing input for a simpler model\n",
    "if inpt.ndim == 1: # if input is vector of size self.M (one time point), expand dims to be (1, M)\n",
    "    inpt = np.expand_dims(inpt, axis=1)\n",
    "inpt=np.tile(inpt,input_dim)\n",
    "\n",
    "# Sample some data from the HMM\n",
    "true_states, obs = true_hmm.sample(time_bins, input=inpt)\n",
    "true_ll = true_hmm.log_likelihood(obs,inputs=inpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d9dd8ea",
   "metadata": {},
   "source": [
    "Below, we first plot the samples generated from the LM-HMM, along the first dimension of input and output, color-coded according to the underlying state. The thin gray lines trace the latent variable as it transitions from one state to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427326ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for k in range(num_states):\n",
    "#     plt.contour(X/X, YY, np.exp(lls[:,k]).reshape(XX.shape), cmap=white_to_color_cmap(colors[k]))\n",
    "    plt.plot(inpt[true_states==k,0], obs[true_states==k, 0], 'o', mfc=colors[k], mec='none', ms=4)\n",
    "    \n",
    "plt.plot(inpt[:,0], obs[:,0], '-k', lw=1, alpha=.05)\n",
    "plt.xlabel(\"input $x_1$\")\n",
    "plt.ylabel(\"obs $y_1$\")\n",
    "plt.title(\"Observation Distributions\")\n",
    "\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_1.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27347df8-c5ff-4e4c-a8eb-b1be50b9605b",
   "metadata": {},
   "source": [
    "## 2. Fit LM-HMM and perform recovery analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c8e9fde-a17a-4022-b2c1-6ba3cb4e7c1e",
   "metadata": {},
   "source": [
    "### 2a. Maximum Likelihood Estimation\n",
    "Now we instantiate a new LM-HMM `mle_hmm` and fit it to the simulated data to recover the ground truth parameters. We will perform a Maximum Likelihood Estimation (MLE) using the Expectation Maximization (EM) algorithm at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563b5e9-f379-4283-a795-29098d60a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_hmm = ssm.HMM(num_states, obs_dim, M=input_dim, observations=\"input_driven_obs_gaussian\", transitions=\"standard\")\n",
    "\n",
    "N_iters = 100 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "fit_ll = mle_hmm.fit(obs, inputs=inpt, method=\"em\", num_iters=N_iters, tolerance=10**-4)\n",
    "\n",
    "# Plot the log probabilities of the true and fit models. Fit model final LL should be greater \n",
    "# than or equal to true LL.\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(fit_ll, label=\"EM\")\n",
    "plt.plot([0, len(fit_ll)], (true_ll) * np.ones(2), ':k', label=\"True\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "# plt.xlim(0, len(fit_ll))\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_EM_LL.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "509b6329-2634-491f-b99c-b7ffd43fd43c",
   "metadata": {},
   "source": [
    "### 2b. Retrieved parameters\n",
    "\n",
    "Compare retrieved weights and transition matrices to generative parameters. To do this, we use the 'most_likely_states method' to infer the most likely latent states given the observations and our model fit (this uses the Viterbi algorithm). Then we find the best permutation of the sequence of 'most_likely_states' from the fit that best correlates with the true state sequence generated from the true generative model.  One way to do this uses the `find_permutation` function from `ssm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea3927-7bfa-468c-9f6a-361f9c330cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_states=mle_hmm.most_likely_states(obs, input=inpt)\n",
    "mle_hmm.permute(find_permutation(true_states, most_likely_states))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85f82d04-c7df-4925-bd3e-331ec5d57a19",
   "metadata": {},
   "source": [
    "Now plot generative and retrieved weights for LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637bbe0-a7db-4a13-8b93-4f94982fcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generative parameters:\n",
    "mus=true_hmm.observations.mus\n",
    "gen_weights=true_hmm.observations.Wks\n",
    "recovered_weights = mle_hmm.observations.Wks\n",
    "recovered_mus=mle_hmm.observations.mus\n",
    "\n",
    "weight_dic={}; weight_dic[0]={}; weight_dic[1]={}; \n",
    "weight_dic[0]['weights']=gen_weights; weight_dic[0]['mus']=mus; weight_dic[0]['label']='true'; \n",
    "weight_dic[1]['weights']=recovered_weights; weight_dic[1]['mus']=recovered_mus; weight_dic[1]['label']='mle'; \n",
    "utilplot.plot_weights_comparison(weight_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dbd16-dba3-49f4-a1f9-9b0594cce228",
   "metadata": {},
   "source": [
    "Now plot generative and retrieved transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf17bdc-f144-4d10-9779-848b8b819be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_trans_mat = np.exp(true_hmm.transitions.log_Ps)\n",
    "recovered_trans_mat = np.exp(mle_hmm.transitions.log_Ps)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "utilplot.plot_trans_matrix(gen_trans_mat)\n",
    "plt.title(\"Generative transition matrix\", fontsize = 15)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "utilplot.plot_trans_matrix(recovered_trans_mat)\n",
    "plt.title(\"Recovered transition matrix\", fontsize = 15)\n",
    "\n",
    "utilplot.plt.subplots_adjust(0, 0, 1, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa41b84c-0e74-4d4d-898f-7b759e8ff167",
   "metadata": {},
   "source": [
    "### 2c. Posterior State Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e452fa-9291-4f54-af86-b11863375a6a",
   "metadata": {},
   "source": [
    "Let's now plot $p(z_{t} = k|\\mathbf{y}, \\{u_{t}\\}_{t=1}^{T})$, the posterior state probabilities, which give the probability of the animal being in state k at trial t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40945c1-e1b8-4623-a0d1-eb411172bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get expected states:\n",
    "posterior_probs = mle_hmm.expected_states(data=obs, input=inpt)[0]\n",
    "posterior_probs0 = np.zeros((true_states.size, true_states.max() + 1))\n",
    "posterior_probs0[np.arange(true_states.size), true_states] = 1\n",
    "\n",
    "\n",
    "print('true')\n",
    "fig=utilplot.plot_postprob_obs(posterior_probs0,obs,inpt,true_hmm,colors,cmap)\n",
    "fig=plt.tight_layout()\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_trial_true.pdf\")\n",
    "print('mle')\n",
    "fig=utilplot.plot_postprob_obs(posterior_probs,obs,inpt,mle_hmm,colors,cmap)\n",
    "fig=plt.tight_layout()\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_trial_recover.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45d30b",
   "metadata": {},
   "source": [
    "### 3b. Stratification for cross-validation (if you have multiple sessions and animals)\n",
    "\n",
    "Stratified folds are created as follows. Each fold contains data from all sessions. Within each session, we split the data into `nKfold` folds, each fold containing consecutive intervals of data. We then collect all intervals corresponding to fold n.1 from all sessions and concatenate it into fold n.1; same for fold n.2, and so on. Each fold now contains data from all sessions. This is achieved with the function `StratifiedKFold`. If you only have one session, you can use the simpler `KFold`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a3003",
   "metadata": {},
   "source": [
    "### 3c. Cross-validation function\n",
    "\n",
    "We define a function that takes training and test sets for each fold, and fits to the training set two LM-HMMs with fixed number of states `num_states`, one using MLE and one using MAP. Then it evaluates the log-likelihood of the test set given MLE and MAP fits. This function takes dictionaries as inputs and returns dictionaries as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5170c8",
   "metadata": {},
   "source": [
    "Now we run the nested cross-validation loop over:\n",
    "- `nKfold` cross-validation folds, stratified as above.\n",
    "- Increasing number of hidden states from $k=1,\\ldots,$`max_states`. \n",
    "- For each fold and each value of hidden states $k$, we fit several `nRunEM` models, each one starting from random initialization of model parameter, to avoid getting stuck in a single local maximum of the likelihood.\n",
    "\n",
    "The code is iterating over `nKfold` normally, but it is parallelized over `max_states`*`nRunEM` to optimize performance. The parallel job is called with:\n",
    "```python\n",
    "results = Parallel(n_jobs=NumThread)(\n",
    "        delayed(xval_func)(data_in, num_states0)\n",
    "        for iRun,num_states0 in zip(RunN,stN))\n",
    "```\n",
    "where `NumThread` is the number of simultaneous jobs we are running, equal to the number of Threads we pick. It is best to not utilize all the cores at disposal on the current machine, but to leave a couple for normal operations. This code runs `xval_func` in parallel, each one using the same `data_in` fold, but iterating over different number of hidden states `num_states0`$=1,\\ldots,`max_states`; and `nRunEM` times to deal with local maxima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create kfold cross-validation object which will split data for us\n",
    "# ntrials=len(obs)\n",
    "# synthetic_data=obs\n",
    "# synthetic_inpts=inpt\n",
    "\n",
    "# nKfold=3\n",
    "# kf = KFold(n_splits=nKfold)\n",
    "# max_states = 5 # largest number of states allowed in the model selection\n",
    "# N_iters = 100 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "# TOL=10**-4 # tolerance parameter (see N_iters)\n",
    "# nRunEM=2 # # of times we run EM for each choice of number of states\n",
    "# NumThread=20\n",
    "# # initialized training and test loglik for model selection, and BIC\n",
    "# ll_training = np.zeros((max_states,nKfold,nRunEM))\n",
    "# ll_heldout = np.zeros((max_states,nKfold,nRunEM))\n",
    "# ll_training_map = np.zeros((max_states,nKfold,nRunEM))\n",
    "# ll_heldout_map = np.zeros((max_states,nKfold,nRunEM))\n",
    "\n",
    "# stN=np.flip(np.tile(np.arange(1,max_states+1),nRunEM))\n",
    "# RunN=np.repeat(np.arange(1,nRunEM+1),max_states,axis=0) \n",
    "\n",
    "\n",
    "# # xval is over cv loops, states, and runs\n",
    "# for iK, (train_index, test_index) in enumerate(kf.split(synthetic_data)):\n",
    "#     nTrain = len(train_index); nTest = len(test_index)#*obs_dim\n",
    "#     data_in={'training_data':synthetic_data[train_index],'test_data':synthetic_data[test_index],\n",
    "#               'training_inpts':synthetic_inpts[train_index],'test_inpts':synthetic_inpts[test_index],\n",
    "#               'N_iters':N_iters,'TOL':TOL}\n",
    "#     results = Parallel(NumThread)(\n",
    "#         delayed(utils_tutorial.xval_func)(data_in, num_states0)\n",
    "#         for iRun,num_states0 in zip(RunN,stN))\n",
    "#     # unpack\n",
    "#     for i in range(max_states*nRunEM):\n",
    "#         ll_training[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_training']\n",
    "#         ll_heldout[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_heldout']\n",
    "#         ll_training_map[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_training_map']\n",
    "#         ll_heldout_map[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_heldout_map']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be190061",
   "metadata": {},
   "source": [
    "### 3d. Information criteria\n",
    "\n",
    "Here we run the BIC and AIC criteria for model selection, which are Bayesian model selection procedure which do not involve cross-valdation. Here, for each value of the hidden states $K$, we fit `nRunEM` different LM-HMMs with MLE, and estimate the average total log-likelihood of the data $LL$ across the `nRunEM` runs. We then plot the $BIC = S*\\ln(T) - 2 LL$, and $AIC = 2*S - 2 LL$ where S is the # of parameters of the model and T is the # of data points. The number of parameters in the model are:\n",
    "- $K(K-1)$ from the transition matrix.\n",
    "- $K-1$ from the initial state distribution.\n",
    "- $K(D*M+D)$ from $W,\\mu$.\n",
    "- $K*D$ from a diagonal noise distribution (our default option); or $K*D(D+1)/2$ for a full noise covariance matrix. \n",
    "\n",
    "In our case then $S=(K+1)(K-1)+K(D*M+2D)$.\n",
    "\n",
    "In general, AIC is a good model selection criterion in the regime of large sample size, and BIC works better for less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create kfold cross-validation object which will split data for us\n",
    "ntrials=len(obs)\n",
    "synthetic_data=obs\n",
    "synthetic_inpts=inpt\n",
    "max_states = 4 # largest number of states allowed in the model selection\n",
    "nRunEM=2\n",
    "NumThread=20\n",
    "# BIC model selection\n",
    "def single_func(synthetic_data,synthetic_inpts):\n",
    "        xval_hmm = ssm.HMM(num_states, obs_dim, M=input_dim, \n",
    "                observations=\"input_driven_obs_gaussian\", transitions=\"standard\")\n",
    "        hmm_lls = xval_hmm.fit(synthetic_data, inputs=synthetic_inpts, method=\"em\", num_iters=N_iters, tolerance=TOL)        \n",
    "        out=xval_hmm.log_likelihood(synthetic_data,inputs=synthetic_inpts)\n",
    "        return out\n",
    "#Number of parameters for the model: (transition matrix) + (mean values for each state) + (covariance matrix for each state)\n",
    "time_bins=len(synthetic_data) # total number of licks\n",
    "BIC = np.zeros((max_states,nRunEM))\n",
    "AIC = np.zeros((max_states,nRunEM))\n",
    "for iS, num_states in enumerate(range(1,max_states+1)):\n",
    "    K = (num_states+1)*(num_states-1) + num_states*(obs_dim*input_dim+2*obs_dim)\n",
    "    for iRun in range(nRunEM):\n",
    "        results = single_func(synthetic_data, synthetic_inpts)\n",
    "        BIC[iS,iRun] = K*np.log(time_bins) - 2*results      \n",
    "        AIC[iS,iRun] = K*2 - 2*results       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f124f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "kindspline='quadratic'\n",
    "# Plot the xval loglik \n",
    "fig = plt.figure(figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "\n",
    "# # reshape variables for plot\n",
    "# ll_training_plot = ll_training.reshape(max_states,nKfold*nRunEM)\n",
    "# ll_heldout_plot = ll_heldout.reshape(max_states,nKfold*nRunEM)\n",
    "# ll_training_map_plot = ll_training_map.reshape(max_states,nKfold*nRunEM)\n",
    "# ll_heldout_map_plot = ll_heldout_map.reshape(max_states,nKfold*nRunEM)\n",
    "\n",
    "\n",
    "# for iS, num_states in enumerate(range(1,max_states+1)):\n",
    "#     plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_training_plot[iS,:], color=colors[0], marker='o',lw=0)\n",
    "#     plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_heldout_plot[iS,:], color=colors[1], marker='o',lw=0)\n",
    "#     # plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_training_map_plot[iS,:], color=colors[2], marker='o',lw=0)\n",
    "#     # plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_heldout_map_plot[iS,:], color=colors[3], marker='o',lw=0)\n",
    "\n",
    "# x=range(1,max_states+1)\n",
    "# y=ll_training_plot.mean(axis=1); error=ll_training_plot.std(axis=1)\n",
    "# plt.plot(x,y, label=\"training_MLE\", color=colors[0])\n",
    "# plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# #\n",
    "# y=ll_heldout_plot.mean(axis=1); error=ll_heldout_plot.std(axis=1)\n",
    "# plt.plot(x,y, label=\"test_MLE\", color=colors[1])\n",
    "# plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# #\n",
    "# # y=ll_training_map_plot.mean(axis=1); error=ll_training_map_plot.std(axis=1)\n",
    "# # plt.plot(x,y, label=\"training_MAP\", color=colors[2])\n",
    "# # plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# # #\n",
    "# # y=ll_heldout_map_plot.mean(axis=1); error=ll_heldout_map_plot.std(axis=1)\n",
    "# # plt.plot(x,y, label=\"test_MAP\", color=colors[3])\n",
    "# # plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.xlabel(\"states\")\n",
    "# plt.xlim(0, max_states+1)\n",
    "# plt.ylabel(\"Log-Likelihood per trial\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "\n",
    "x=range(1,max_states+1); \n",
    "y=np.mean(BIC,1); error=np.std(BIC,1)\n",
    "plt.plot(x,y, label=\"BIC\")\n",
    "plt.fill_between(x, y-error, y+error,\n",
    "    alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "y=np.mean(AIC,1); error=np.std(AIC,1)\n",
    "plt.plot(x,y, label=\"AIC\")\n",
    "plt.xlabel(\"states\")\n",
    "plt.xlim(0, max_states+1)\n",
    "plt.ylabel(\"criterion\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "\n",
    "# savefile= 'HMM_model_sel_concAllBouts.pdf'\n",
    "# plt.savefig(savefile, format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.close(fig) \n",
    "plt.tight_layout()\n",
    "\n",
    "model_sel={'ll_training':ll_training,'ll_heldout':ll_heldout,'ll_training_map':ll_training_map,'ll_heldout_map':ll_heldout_map,'BIC':BIC}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2dce34",
   "metadata": {},
   "source": [
    "# Fit LM-HMM to mystery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_data=y_new.reshape(-1,1)\n",
    "# synthetic_inpts=X_new.reshape(-1,1)\n",
    "\n",
    "# max_states = 5 # largest number of states allowed in the model selection\n",
    "# N_iters = 100 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "# TOL=10**-4 # tolerance parameter (see N_iters)\n",
    "# nRunEM=4 # # of times we run EM for each choice of number of states\n",
    "# # NumThread=20\n",
    "# # initialized training and test loglik for model selection, and BIC\n",
    "# ll_training = np.zeros((max_states,nKfold,nRunEM))\n",
    "# ll_heldout = np.zeros((max_states,nKfold,nRunEM))\n",
    "# ll_training_map = np.zeros((max_states,nKfold,nRunEM))\n",
    "# ll_heldout_map = np.zeros((max_states,nKfold,nRunEM))\n",
    "\n",
    "# stN=np.flip(np.tile(np.arange(1,max_states+1),nRunEM))\n",
    "# RunN=np.repeat(np.arange(1,nRunEM+1),max_states,axis=0) \n",
    "\n",
    "# # xval is over cv loops, states, and runs\n",
    "# for iK, (train_index, test_index) in enumerate(kf.split(synthetic_data)):\n",
    "#     nTrain = len(train_index); nTest = len(test_index)#*obs_dim\n",
    "#     data_in={'training_data':synthetic_data[train_index],'test_data':synthetic_data[test_index],\n",
    "#               'training_inpts':synthetic_inpts[train_index],'test_inpts':synthetic_inpts[test_index],\n",
    "#               'N_iters':N_iters,'TOL':TOL}\n",
    "#     results = Parallel(-1)(delayed(utils_tutorial.xval_func)(data_in, num_states0)\n",
    "#         for iRun,num_states0 in zip(RunN,stN))\n",
    "#     # unpack\n",
    "#     for i in range(max_states*nRunEM):\n",
    "#         ll_training[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_training']\n",
    "#         ll_heldout[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_heldout']\n",
    "#         ll_training_map[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_training_map']\n",
    "#         ll_heldout_map[stN[i]-1,iK,RunN[i]-1]=results[i]['ll_heldout_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5de6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data=y_new.reshape(-1,1)\n",
    "synthetic_inpts=X_new.reshape(-1,1)\n",
    "\n",
    "max_states = 4 # largest number of states allowed in the model selection\n",
    "N_iters = 100 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "TOL=10**-4 # tolerance parameter (see N_iters)\n",
    "nRunEM=2 # # of times we run EM for each choice of number of states\n",
    "NumThread=20\n",
    "# \n",
    "# BIC model selection\n",
    "def single_func(synthetic_data,synthetic_inpts):\n",
    "        xval_hmm = ssm.HMM(num_states, obs_dim, M=input_dim, \n",
    "                observations=\"input_driven_obs_gaussian\", transitions=\"standard\")\n",
    "        hmm_lls = xval_hmm.fit(synthetic_data, inputs=synthetic_inpts, method=\"em\", num_iters=N_iters, tolerance=TOL)        \n",
    "        out=xval_hmm.log_likelihood(synthetic_data,inputs=synthetic_inpts)\n",
    "        return out\n",
    "#Number of parameters for the model: (transition matrix) + (mean values for each state) + (covariance matrix for each state)\n",
    "time_bins=len(synthetic_data) # total number of licks\n",
    "BIC = np.zeros((max_states,nRunEM))\n",
    "AIC = np.zeros((max_states,nRunEM))\n",
    "for iS, num_states in enumerate(range(1,max_states+1)):\n",
    "    K = (num_states+1)*(num_states-1) + num_states*(obs_dim*input_dim+2*obs_dim)\n",
    "    for iRun in range(nRunEM):\n",
    "        results = single_func(synthetic_data, synthetic_inpts)\n",
    "        BIC[iS,iRun] = K*np.log(time_bins) - 2*results      \n",
    "        AIC[iS,iRun] = K*2 - 2*results           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5336b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "kindspline='quadratic'\n",
    "# Plot the xval loglik \n",
    "fig = plt.figure(figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "\n",
    "# # reshape variables for plot\n",
    "# ll_training_plot = ll_training.reshape(max_states,nKfold*nRunEM)\n",
    "# ll_heldout_plot = ll_heldout.reshape(max_states,nKfold*nRunEM)\n",
    "# ll_training_map_plot = ll_training_map.reshape(max_states,nKfold*nRunEM)\n",
    "# ll_heldout_map_plot = ll_heldout_map.reshape(max_states,nKfold*nRunEM)\n",
    "\n",
    "\n",
    "# for iS, num_states in enumerate(range(1,max_states+1)):\n",
    "#     plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_training_plot[iS,:], color=colors[0], marker='o',lw=0)\n",
    "#     plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_heldout_plot[iS,:], color=colors[1], marker='o',lw=0)\n",
    "#     plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_training_map_plot[iS,:], color=colors[2], marker='o',lw=0)\n",
    "#     plt.plot((iS+1)*np.ones(nKfold*nRunEM),ll_heldout_map_plot[iS,:], color=colors[3], marker='o',lw=0)\n",
    "\n",
    "# x=range(1,max_states+1)\n",
    "# y=ll_training_plot.mean(axis=1); error=ll_training_plot.std(axis=1)\n",
    "# plt.plot(x,y, label=\"training_MLE\", color=colors[0])\n",
    "# plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# #\n",
    "# y=ll_heldout_plot.mean(axis=1); error=ll_heldout_plot.std(axis=1)\n",
    "# plt.plot(x,y, label=\"test_MLE\", color=colors[1])\n",
    "# plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# #\n",
    "# y=ll_training_map_plot.mean(axis=1); error=ll_training_map_plot.std(axis=1)\n",
    "# plt.plot(x,y, label=\"training_MAP\", color=colors[2])\n",
    "# plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# #\n",
    "# y=ll_heldout_map_plot.mean(axis=1); error=ll_heldout_map_plot.std(axis=1)\n",
    "# plt.plot(x,y, label=\"test_MAP\", color=colors[3])\n",
    "# plt.fill_between(x,y-error, y+error,alpha=0.1)\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.xlabel(\"states\")\n",
    "# plt.xlim(0, max_states+1)\n",
    "# plt.ylabel(\"Log-Likelihood per trial\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "\n",
    "x=range(1,max_states+1); \n",
    "y=np.mean(BIC,1); error=np.std(BIC,1)\n",
    "plt.plot(x,y, label=\"BIC\")\n",
    "plt.fill_between(x, y-error, y+error,\n",
    "    alpha=0.5, edgecolor='#CC4F1B', facecolor='#FF9848')\n",
    "y=np.mean(AIC,1); error=np.std(AIC,1)\n",
    "plt.plot(x,y, label=\"AIC\")\n",
    "plt.xlabel(\"states\")\n",
    "plt.xlim(0, max_states+1)\n",
    "plt.ylabel(\"criterion\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "\n",
    "# savefile= 'HMM_model_sel_concAllBouts.pdf'\n",
    "# plt.savefig(savefile, format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.close(fig) \n",
    "plt.show()\n",
    "\n",
    "model_sel={'ll_training':ll_training,'ll_heldout':ll_heldout,'ll_training_map':ll_training_map,'ll_heldout_map':ll_heldout_map,'BIC':BIC}\n",
    "\n",
    "\n",
    "best_k=utils_tutorial.find_elbow_point(-np.mean(BIC,1))+1\n",
    "print(f'The best number of mixture components is {best_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7eb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states=best_k\n",
    "obs_dim=1\n",
    "input_dim=1\n",
    "mle_hmm = ssm.HMM(num_states, obs_dim, M=input_dim, observations=\"input_driven_obs_gaussian\", transitions=\"standard\")\n",
    "\n",
    "obs=y_new.reshape(-1,1)\n",
    "inpt=X_new.reshape(-1,1)\n",
    "N_iters = 50 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "fit_ll = mle_hmm.fit(obs, inputs=inpt, method=\"em\", num_iters=N_iters, tolerance=10**-4)\n",
    "\n",
    "# Plot the log probabilities of the true and fit models. Fit model final LL should be greater \n",
    "# than or equal to true LL.\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(fit_ll, label=\"EM\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "# plt.xlim(0, len(fit_ll))\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fit parameters:\n",
    "mus=mle_hmm.observations.mus\n",
    "gen_weights=mle_hmm.observations.Wks\n",
    "sigmas=np.sqrt(mle_hmm.observations.Sigmas)\n",
    "trans0=np.exp(mle_hmm.transitions.log_Ps)\n",
    "print('trans\\n',trans0)\n",
    "print('mus\\n',mus)\n",
    "print('Wks\\n',gen_weights)\n",
    "print('Sigmas\\n',sigmas)\n",
    "\n",
    "utilplot.plot_weights(gen_weights,mus)\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_params.pdf\")\n",
    "    \n",
    "fig = plt.figure(figsize=(5, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "utilplot.plot_trans_matrix(trans0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ec867",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=len(obs)\n",
    "posterior_probs = mle_hmm.expected_states(data=obs, input=inpt)[0]\n",
    "most_likely_states=mle_hmm.most_likely_states(obs, input=inpt)\n",
    "plt.figure(figsize=(10, 4))\n",
    "for k in range(num_states):\n",
    "    plt.plot(inpt[most_likely_states==k,0], obs[most_likely_states==k, 0], 'o',linestyle='', mfc=colors[k], mec='none', ms=4,label=f\"State {k+1}\")\n",
    "    \n",
    "# r2\n",
    "pred_obs=np.zeros_like(obs)\n",
    "# for k in range(num_states):\n",
    "#     pred_obs[most_likely_states==k]=inpt[most_likely_states==k,:]*gen_weights[k]+mus[k]\n",
    "pred_w=np.matmul(posterior_probs,gen_weights.reshape(num_states,-1)).reshape(n_samples,obs_dim,input_dim)\n",
    "pred_wx = np.einsum('ijk,ik->ij', pred_w, inpt)\n",
    "pred_obs=pred_wx+np.matmul(posterior_probs,mus.reshape(num_states,-1)).reshape(-1,obs_dim)\n",
    "r2=r2_score(obs, pred_obs)\n",
    "\n",
    "# plt.plot(inpt[:,0], obs[:,0], '-k', lw=1, alpha=.25)\n",
    "plt.xlabel(\"input $x_1$\")\n",
    "plt.ylabel(\"obs $y_1$\")\n",
    "plt.title(f\"Observation Distributions, r2={r2}\")\n",
    "\n",
    "# if save_figures:\n",
    "#     plt.savefig(\"hmm_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responsibilities and labels\n",
    "posterior_probs = mle_hmm.expected_states(data=obs, input=inpt)[0]\n",
    "predicted_labels = most_likely_states\n",
    "xlim=400\n",
    "\n",
    "y=obs.flatten()\n",
    "X=inpt.flatten()\n",
    "# Fit the model to the generated data\n",
    "n_components=num_states\n",
    "# Plot the time series of the data (obs and inputs)\n",
    "plt.figure(figsize=(14, 8))\n",
    "n_samples = len(y)\n",
    "# Plot observations and inputs\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(n_samples), y, label='Observations', color='black')\n",
    "plt.plot(range(n_samples), X, label='Inputs', color='gray')\n",
    "\n",
    "for i in range(n_components):\n",
    "    plt.fill_between(range(n_samples), y, where=(posterior_probs[:, i] > 0.5), color=colors[i], alpha=0.3, label=f'Mixture Component {i+1}')\n",
    "\n",
    "plt.xlabel('Time (Sample Number)')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Time Series of Observations and Inputs with posterior_probs')\n",
    "plt.xlim(0,xlim)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot posterior_probs\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range(n_components):\n",
    "    plt.plot(range(n_samples), posterior_probs[:, i], label=f'posterior_probs {i+1}', color=colors[i])\n",
    "\n",
    "plt.xlabel('Time (Sample Number)')\n",
    "plt.ylabel('Responsibility')\n",
    "plt.title('posterior_probs of Each Mixture Component Over Time')\n",
    "plt.xlim(0,xlim)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the predicted labels\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "indplot = np.arange(n_samples)\n",
    "for i in range(n_components):\n",
    "    mask = (predicted_labels == i)\n",
    "    plt.scatter(indplot[mask], y[mask], color=colors[i], label=f'Label {i}')\n",
    "plt.xlabel('Time (Sample Number)')\n",
    "plt.ylabel('Observations')\n",
    "plt.title('Predicted Labels for Each Sample')\n",
    "plt.xlim(0,xlim)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "for i in range(posterior_probs.shape[1]):\n",
    "    plt.subplot(1,posterior_probs.shape[1],  i+1)\n",
    "    plt.hist(posterior_probs[:, i], color=colors[i], label=f'p={i+1}')\n",
    "    plt.legend()\n",
    "    plt.title(f'posterior_probs {i+1}')\n",
    "    plt.xlabel('posterior_probs'); plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a876791",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 6))\n",
    "xlim=300\n",
    "timescale = []\n",
    "for i in range(n_components):\n",
    "          y_acf=utils_tutorial.autocovariance(posterior_probs[:,i])\n",
    "          y_acf=y_acf[2:]/np.max(y_acf[2:])\n",
    "          timescale.append( int(utils_tutorial.compute_hwhm(y_acf)))\n",
    "          plt.plot(y_acf, label=f\"Timescale: {timescale[i]}\")\n",
    "          plt.axhline(y_acf[timescale[i]], xmin=0, xmax=(timescale[i])/xlim, color='red', linestyle='--')\n",
    "plt.axhline(0, color='black')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.title(\"Autocorrelation of Time Series with Timescales\")\n",
    "plt.xlim([0, xlim])\n",
    "plt.show()\n",
    "# plt.savefig('AutocorrTimescales.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f738b0",
   "metadata": {},
   "source": [
    "# Bonus material"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcac21dc",
   "metadata": {},
   "source": [
    "## 2.d Maximum A Posteriori (MAP) estimation\n",
    "\n",
    "Above, we performed Maximum Likelihood Estimation to retrieve the generative parameters of the LM-HMM. If the number of trials is small, it is useful to introduce a prior distribution on the model parameters and perform Maximum A Posteriori (MAP) estimation. The relative weight of the prior compared to the likelihood decreases as the sample size increases, so that for very large sample size the MAP and MLE estimations should converge. For small sample size, however, the MAP estimation will significantly differ from the MLE in the direction of the constraints imposed by the chosen prior distributions on the weights. The prior we consider for the LM-HMM is the product of a Gaussian prior on the LM weights, and a Dirichlet prior on the transition matrix, :\n",
    "\n",
    " The prior we consider for the LM-HMM is the product of a Gaussian prior on the LM weights, $W$, and a Dirichlet prior on the transition matrix, $A$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr(W,A) &= {\\it N}(W|0, \\Sigma)Dir(A|\\alpha) \\\\&= {\\it N}(W|0, diag(\\sigma^{2}, \\cdots, \\sigma^{2})) \\prod_{j=1}^{K} \\dfrac{1}{B(\\alpha)} \\prod_{k=1}^{K} A_{jk}^{\\alpha -1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There are two hyperparameters controlling the strength of the prior: $\\sigma$ and $\\alpha$.  The larger the value of $\\sigma$ and if $\\alpha = 1$, the more similar MAP estimation will become to Maximum Likelihood Estimation, and the prior term will become an additive offset to the objective function of the LM-HMM that is independent of the values of $W$ and $A$.  In comparison, setting $\\sigma = 2$ and $\\alpha = 2$ will result in the prior no longer being independent of $W$ and $\\alpha$.  \n",
    "\n",
    "In order to perform MAP estimation for the GLM-HMM with `ssm`, the new syntax is:\n",
    "\n",
    "```python\n",
    "map_hmm = ssm.HMM(num_states, obs_dim, M=input_dim, \n",
    "          observations=\"input_driven_obs_gaussian\", \n",
    "          observation_kwargs=dict(prior_sigma=prior_sigma),\n",
    "          transitions=\"sticky\", transition_kwargs=dict(alpha=prior_alpha,kappa=0))\n",
    "```\n",
    "\n",
    "where `prior_sigma` is the $\\sigma$ parameter from above, and `prior_alpha` is the $\\alpha$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfb9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_sigma = 1\n",
    "prior_alpha = 1\n",
    "\n",
    "map_hmm = ssm.HMM(num_states, obs_dim, M=input_dim, \n",
    "          observations=\"input_driven_obs_gaussian\", \n",
    "          observation_kwargs=dict(prior_sigma=prior_sigma),\n",
    "          transitions=\"sticky\", transition_kwargs=dict(alpha=prior_alpha,kappa=0))\n",
    "\n",
    "N_iters = 10000 # maximum number of EM iterations. Fitting with stop earlier if increase in LL is below tolerance specified by tolerance parameter\n",
    "fit_ll_map = map_hmm.fit(obs, inputs=inpt, method=\"em\", num_iters=N_iters, tolerance=10**-6)\n",
    "\n",
    "# match states with ground truth\n",
    "most_likely_states_map=map_hmm.most_likely_states(obs, input=inpt)\n",
    "map_hmm.permute(find_permutation(true_states, most_likely_states_map))\n",
    "\n",
    "# Plot the log likelihood of the true and fit models. Fit model final LL should be greater \n",
    "# than or equal to true LL.\n",
    "fig = plt.figure(figsize=(4, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(fit_ll_map, label=\"EM\")\n",
    "plt.plot([0, len(fit_ll_map)], (true_ll) * np.ones(2), ':k', label=\"True\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"EM Iteration\")\n",
    "# plt.xlim(0, len(fit_ll))\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "613271bb",
   "metadata": {},
   "source": [
    "Compare true likelihood with MLE and MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_likelihood = true_hmm.log_likelihood(obs, inputs=inpt)\n",
    "mle_final_ll = mle_hmm.log_likelihood(obs, inputs=inpt) \n",
    "map_final_ll = map_hmm.log_likelihood(obs, inputs=inpt) \n",
    "# Plot these values\n",
    "fig = plt.figure(figsize=(2, 2.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "loglikelihood_vals = [true_likelihood, mle_final_ll, map_final_ll]\n",
    "colors_ll = ['Red', 'Navy', 'Purple']\n",
    "for z, occ in enumerate(loglikelihood_vals):\n",
    "    plt.bar(z, occ, width = 0.8, color = colors_ll[z])\n",
    "plt.xticks([0, 1, 2], ['true', 'mle', 'map'], fontsize = 10)\n",
    "plt.xlabel('model', fontsize = 15)\n",
    "plt.ylabel('loglikelihood', fontsize=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2f6193e",
   "metadata": {},
   "source": [
    "Compare recovered parameters with MLE and MAP estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238219a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generative parameters:\n",
    "mus=true_hmm.observations.mus\n",
    "a=true_hmm.observations.Wks\n",
    "recovered_weights_mle = mle_hmm.observations.Wks\n",
    "recovered_mus_mle=mle_hmm.observations.mus\n",
    "recovered_weights_map = map_hmm.observations.Wks\n",
    "recovered_mus_map=map_hmm.observations.mus\n",
    "\n",
    "\n",
    "weight_dic={}; weight_dic[0]={}; weight_dic[1]={};  weight_dic[2]={}; \n",
    "weight_dic[0]['weights']=gen_weights; weight_dic[0]['mus']=mus; weight_dic[0]['label']='true'; \n",
    "weight_dic[1]['weights']=recovered_weights_mle; weight_dic[1]['mus']=recovered_mus_mle; weight_dic[1]['label']='mle'; \n",
    "weight_dic[2]['weights']=recovered_weights_map; weight_dic[2]['mus']=recovered_mus_map; weight_dic[2]['label']='map'; \n",
    "utilplot.plot_weights_comparison(weight_dic)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9820b630b875be862b66b07d5862921c0ec71ae4e168b508369c3d78b8dd6e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ssm_conda_mazzu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
